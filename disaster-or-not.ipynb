{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D,Convolution1D,Dropout,MaxPooling1D,Conv1D,Input,Flatten\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading & Exploring dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('training data shape', tweet.shape)\nprint('test data shape', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def explore_data(df):\n    \n    '''Input- df= pandas dataframes to be explored\n       Output- print shape, info and first 5 records of the dataframe \n    '''\n    \n    print(\"-\"*50)\n    print('Shape of the dataframe:',df.shape)\n    print(\"Number of records in train data set:\",df.shape[0])\n    print(\"Information of the dataset:\")\n    df.info()\n    print(\"-\"*50)\n    print(\"First 5 records of the dataset:\")\n    return df.head()\n    print(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets use explore_data() function to explore train data\nexplore_data(tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets use explore_data() function to explore test data\nexplore_data(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Class Distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfeature='target'\nsns.countplot(feature, data=tweet)\nprint('Target of 0 is {} % of total'.format(round(tweet[feature].value_counts()[0]/len(tweet[feature])*100)))\nprint('Target of 1 is {} % of total'.format(round(tweet[feature].value_counts()[1]/len(tweet[feature])*100)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Pre-processing\n\nBefore starting any NLP project, text data needs to be pre-processed to convert it into in a consistent format.Text will be cleaned, tokneized and converted into a matrix.\n\nSome of the basic text pre-processing techniques includes:\n1. **Make text all lower or uppercase**\n2. **Removing Noise** - Remove Punctuation and numerical Values\n3. **Tokenization**  - Process of converting the normal text strings into a list of tokens i.e. words.\n4. **Stopword Removal**-Some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely.\n5. **Stemming**-Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form — generally a written word form. Example if we were to stem the following words: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, the result would be a single word “stem”.\n6. **Lemmatization**-A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that “run” is a base form for words like “running” or “ran” or that the word “better” and “good” are in the same lemma so they are considered the same.\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a function to clean the text\ndef clean_text(text):\n\n    '''\n    Input- 'text' to be cleaned\n       \n       Output- Convert input 'text' to lowercase,remove square brackets,links,punctuation\n       and words containing numbers. Return clean text.\n    \n    '''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_df1=tweet.copy()\ntest_df1=test.copy()\ntweet_df1['text'] = tweet_df1['text'].apply(lambda x: clean_text(x))\ntest_df1['text'] = test_df1['text'].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets look at cleaned data\ndef text_after_preprocess(before_text,after_text):\n    \n    '''\n    Input- before_text=text column before cleanup\n              after_text= text column after cleanup\n       Output- print before and after text to compare how it looks after cleanup\n       \n    '''\n    print('-'*60)\n    print('Text before cleanup')\n    print('-'*60)\n    print(before_text.head(5))\n    print('-'*60)\n    print('Text after cleanup')\n    print('-'*60)\n    print(after_text.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_after_preprocess(tweet.text,tweet_df1.text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_after_preprocess(test.text,test_df1.text)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n# Example how tokenization of text works\ntext = \"Heard about #earthquake is different cities, stay safe everyone.\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\nprint(\"-\"*100)\nprint(\"Example Text: \",text)\nprint(\"-\"*100)\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#before tokenization\ntweet_df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets Tokenize the training and the test dataset copies with RegEx tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntweet_df1['text'] = tweet_df1['text'].apply(lambda x: tokenizer.tokenize(x))\ntest_df1['text'] = test_df1['text'].apply(lambda x: tokenizer.tokenize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets check tokenized text\ntweet_df1['text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stop words Removal","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a funtion to remove stopwords\ndef remove_stopwords(text):\n    \n    \"\"\"\n    Input- text=text from which english stopwprds will be removed\n    Output- return text without english stopwords \n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Before stopwords removal\ntweet_df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_df1['text'] = tweet_df1['text'].apply(lambda x : remove_stopwords(x))\ntest_df1['text'] = test_df1['text'].apply(lambda x : remove_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#after stopwords removal\ntweet_df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stemming and Lemmatization\n\nStemming and lemmatization sometimes doesnt necessarily improve results as sometimes we dont want to trim words rather preserve their original form.Its usage from problem to problem and for this problem it wouldnt be good idea to use it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stemming and Lemmatization examples\ntext =  [ 'deduced', 'dogs', 'talking', 'studies']\ndef Stemming_Lemmatizing(text):\n    # Lemmatizer\n    lemmatizer=nltk.stem.WordNetLemmatizer()\n    words=[lemmatizer.lemmatize(token) for token in text]\n    \n    # Stemmer\n    stemmer = nltk.stem.PorterStemmer()\n    words=[stemmer.stem(token) for token in text]\n\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Stemming_Lemmatizing(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tweet_df1['text'] = tweet_df1['text'].apply(lambda x : Stemming_Lemmatizing(x))\n#test_df1['text'] = test_df1['text'].apply(lambda x : Stemming_Lemmatizing(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert the text list into string","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def listToString(s):  \n    \n    # initialize an empty string \n    str1 = \" \" \n    \n    # return string   \n    return (str1.join(s)) \n        \n        \n# Driver code     \ns = ['Geeks', 'for', 'Geeks'] \nprint(listToString(s))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tweet_df1['text'] = tweet_df1['text'].apply(lambda x : listToString(x))\n#test_df1['text'] = test_df1['text'].apply(lambda x : listToString(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Spell Checker","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspellchecker\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_df1['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.concat([tweet_df1,test_df1])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GloVe for Vectorization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in tweet if((word.isalpha()==1) )]\n        corpus.append(words)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=create_corpus(df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_pad.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create an Embedding Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words=len(word_index)+1\n\nembedding_matrix=np.zeros((num_words,200))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initiating TPU ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a Baseline Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    model=Sequential()\n\n    embedding=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\n    model.add(embedding)\n    model.add(SpatialDropout1D(0.2))\n    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n\n\n    optimzer=Adam(learning_rate=1e-5)\n\n    model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split  training and validation set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train,y_train,batch_size=16,epochs=15,validation_data=(X_test,y_test),verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a callback Function\nfrom keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\ncallbacks = [\n             ReduceLROnPlateau(monitor='val_accuracy', \n                               factor=0.2, \n                               patience=3, \n                               verbose=1)]\n\nfilter_length1 = 3\nfilter_length2 = 5\ndropout=0.5\nnb_filter = 64\nlearning_rate=3e-3\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding a Cnn layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    model=Sequential()\n\n    embedding=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\n    model.add(embedding)\n    model.add(SpatialDropout1D(0.2))\n   \n    model.add(Conv1D(64, 5,padding = 'same', activation='relu'))\n    model.add(MaxPooling1D(pool_size=4))\n    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n\n\n\n\n    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate),metrics=['accuracy'])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train,y_train,batch_size=8,epochs=30,validation_data=(X_test,y_test),verbose=True,callbacks = callbacks)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nmodel1 = keras.models.Sequential([\n    keras.layers.Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LEN,trainable=False),\n    keras.layers.LSTM(100,return_sequences=True),\n    keras.layers.LSTM(200),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1,activation='sigmoid')\n])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.compile(\n  loss='binary_crossentropy',\n  optimizer='adam',\n  metrics=['accuracy'],\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history1 = model1.fit(X_train,y_train,\n                    batch_size=64,\n                    epochs=10,\n                    validation_split=0.2\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(history1.history['accuracy'], label='train')\nplt.plot(history1.history['val_accuracy'], label='test')\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = keras.models.Sequential([\n    keras.layers.Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LEN,trainable=False),\n    keras.layers.GRU(100,return_sequences=True),\n    keras.layers.GRU(200),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1,activation='sigmoid')\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.compile(\n  loss='binary_crossentropy',\n  optimizer='adam',\n  metrics=['accuracy'],\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history2 = model2.fit(X_train,y_train,\n                    batch_size=64,\n                    epochs=10,\n                    validation_split=0.2\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = keras.models.Sequential([\n    keras.layers.Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LEN,trainable=False),\n    keras.layers.Bidirectional(keras.layers.LSTM(100,return_sequences=True)),\n    keras.layers.Bidirectional(keras.layers.LSTM(200)),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1,activation='sigmoid')\n])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3.compile(\n  loss='binary_crossentropy',\n  optimizer='adam',\n  metrics=['accuracy'],\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history3 = model3.fit(X_train,y_train,\n                    batch_size=64,\n                    epochs=10,\n                    validation_split=0.2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pre=model1.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub1=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub1.to_csv('submission1.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pre=model3.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub3=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub3.to_csv('submission3.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}